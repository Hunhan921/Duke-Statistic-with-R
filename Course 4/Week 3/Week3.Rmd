# **More Probability and Hypothesis Testing review**

```{r load-packages, message=FALSE}
library(statsr)
```

**Expected value**

*Example*

Suppose we have a bag of 5 M&M's. The general percentage of yellow M&M's in a bag is
10% i.e., 0.1. Let X be the number of yellow M&M's in this bag. What is the expected number of yellow
M&M's we would have?

```{r}
#possible values that X can take
values = 0:5
#probability for each value
probs = dbinom(values, size = 5, prob = 0.1)
#expected value 
expected = sum(values*probs)
expected
```

0.5 happens to be the product of the total number n = 5 and the probability of success p = 0.1. This is not
a coincident, as in the table we provided in last week’s review, the mean of a Binomial distribution of np.
Here we just have shown you a particular case.
We will use expected value to calculate the expected loss associated with different hypothesis after seeing
the data in this week’s Decision Making Module.

**Joint (Probability) Distribution (new)**

When we have more than 1 random variable, say X and Y . The two may not be independent. Instead
they may correlate to each other in some sense. If we want to consider them together as a pair, we need to
introduce a “joint” probability distribution that captures the behavior of both X and Y .
For example, when considering to assign prior distribution to the parameters µ and σ of the Normal distribution, if we do not know σ, the Normal-Normal conjugate family cannot be used. Then we would need to
consider a joint distribution of both µ and σ.

In the discrete case, the joint distribution of discrete random variables X and Y , is given by

p(k, l) = P(X = k and Y = l).

Recall that, in general P(X = k and Y = l) 6= P(X = k) × P(Y = l). (X and Y may not be independent!)
When one or more of the random variables are continuous, the joint distribution will be more complicated
and cannot be easily calculated. We usually denote it as p(x, y).

**Marginal Distribution (new)**

While we often cannot obtain the joint distribution p(x, y) from the distribution p(x) and p(y), the other way
is possible. That is, we can recover the distributions of X and Y : p(x) and p(y) from the joint distribution.
And they are called the marginal distributions. Marginal distribution gives the probabilities of various values
of the variables in the subset **without** reference to the values of the other variables. This contrasts with
conditional probability, where we get the probability of the variables based on the values of other variables.

We may use the integrate function in R to help us to perform integrals.

**Student’s t-Distribution**

In Course 2 **Inferential Statistics**, we used Student’s t-distribution for inference of means. We say, only
when we know the population standard deviation σ and the data pass all the assumptions, that we can use
the z-score. Otherwise, we need the t-score to help us in hypothesis testings.

Here, we also provide you the formula of the Student’s t-distribution, and show you how the t-score standardize the t-distribution. You do not need to be proficient with these details, because we have R functions
to help us. 

This Student’s t-distribution is centered at 0 (the location parameter), with a scale parameter equal to 1, and degree of freedom parameter ν.

Compared to the Normal distribution, the curve of the Student’s t-distribution has slightly heavier tails, and
therefore, it is a little “shorter” in the middle.

**Cauchy distribution (new)**

A special t-distribution we will also use in Week 3 is called the Cauchy distribution. Cauchy distribution is
the t-distribution when the degree of freedom ν = 1

C(m, σ2) = t(1, m, σ2)


Since t-distribution involves special function Γ(·) and complicated arithmetics, we will not expect you to
know how to do the calculation by hands. R provides us the function dt and dcauchy for the standardized
t-distribution and the general Cauchy distribution. To get the non-standardized t-distribution from the
standardized one, we need to do the transformation

x = m + t × σ, (location + t × scale)

```{r}
x = seq(-8, 10, by = 0.005)
# standardized t-distribution with degree of freedom 4
y_t_standard = dt(x, df = 4)
# non-standardized t-distribution with location 1, scale 2, degree of freedom 4
y_t_general = dt((x - 1) / 2, df = 4)
# Cauchy distribution with location 1, scale 2
y_cauchy = dcauchy(x, location = 1, scale = 2)

```

**Hypothesis Test for Means - Frequent Inference**

*Hypothesis Test for Mean from One Sample*

Under R, we can use the t.test function from the stats package to perform this test. Here x is the vector
containing all the observations of the variable X of interest, mu_0 is the inferred value µ0. And we choose
alternative = "two.sided" for a two-sided test.

t.test(x, alternative = "two.sided", mu = mu_0, conf.level = 0.95)

*Hypothesis Test for Two Paired Means*

diff is the difference between the two vectors x1 and x2..

diff = x1 - x2

t.test(diff, alternative = "two.sided", mu = 0, conf.level = 0.95)

*Hypothesis Test for Two Independent Means*

Suppose we are interested in comparing vectors x1 and x2. Then the test can be done by

t.test(x1, x2, alternative = "two.sided", mu = 0, conf.level = 0.95)

**Problem with p-Value**

